# trainer.py

import torch
from torch.utils.data import DataLoader
from transformers import (
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)

from constants import OUTPUT_DIR
from evaluation import compute_metrics


def create_training_arguments() -> Seq2SeqTrainingArguments:
    """
    Create and return the training arguments for the model.

    Ð ÐµÐ¶Ð¸Ð¼ "inference only":
    - train() Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð´ÐµÐ»Ð°ÐµÑ‚ (ÑÐ¼. InferenceOnlyTrainer),
    - Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð½ÑƒÐ¶Ð½Ñ‹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ evaluate(),
    - Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð»Ð¸Ð½Ñƒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð»Ð¾Ð²Ð¸Ñ‚ÑŒ OOM.
    """
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,

        # Ð¢Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð½Ðµ Ð±ÑƒÐ´ÐµÑ‚, Ð½Ð¾ Trainer Ð²ÑÑ‘ Ñ€Ð°Ð²Ð½Ð¾ Ð¿Ñ€Ð¾ÑÐ¸Ñ‚ ÑÑ‚Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹.
        num_train_epochs=1,
        max_steps=0,  # Ñ„Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸ "Ð½ÐµÑ‚ ÑˆÐ°Ð³Ð¾Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"

        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,

        learning_rate=2e-4,
        weight_decay=0.01,
        warmup_steps=0,

        logging_steps=1,
        save_steps=50,

        # Ð’ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐµ "Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ" Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ Ð½Ðµ Ð³Ð¾Ð½ÑÐµÐ¼
        evaluation_strategy="no",

        save_total_limit=1,
        load_best_model_at_end=False,

        metric_for_best_model="bleu",
        greater_is_better=True,

        max_grad_norm=1.0,

        # ÐœÑ‹ Ð¡ÐÐœÐ˜ Ð±ÑƒÐ´ÐµÐ¼ Ð²Ñ‹Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ generate() Ð² ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½Ð¾Ð¹ evaluate,
        # Ñ‚Ð°Ðº Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾Ñ‚ Ñ„Ð»Ð°Ð³ Trainer'Ð° Ð½Ð°Ð¼ Ð½Ðµ Ð²Ð°Ð¶ÐµÐ½, Ð½Ð¾ Ð¿ÑƒÑÑ‚ÑŒ Ð±ÑƒÐ´ÐµÑ‚.
        predict_with_generate=True,

        # Ð”Ð»Ñ 3.3B-Ð¼Ð¾Ð´ÐµÐ»Ð¸: fp16, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²Ð»ÐµÐ·Ñ‚ÑŒ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ
        fp16=True,
        gradient_accumulation_steps=1,

        # Ð’ÐÐ–ÐÐž: gradient_checkpointing Ð²Ñ‹ÐºÐ»ÑŽÑ‡ÐµÐ½, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð±Ñ‹Ð»Ð¾ ÑÐ¸Ð³Ð½Ð°Ñ‚ÑƒÑ€Ð½Ñ‹Ñ… Ð±Ð°Ð³Ð¾Ð².
        gradient_checkpointing=False,

        dataloader_num_workers=4,

        # Ð§Ñ‚Ð¾Ð±Ñ‹ Trainer Ð½Ðµ Ð¿Ñ‹Ñ‚Ð°Ð»ÑÑ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð² WandB Ð¸ Ñ‚.Ð¿.
        report_to="none",

        # Ð¥Ð¾Ñ‚Ð¸Ð¼ Ð²Ð¸Ð´ÐµÑ‚ÑŒ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ, ÐµÑÐ»Ð¸ Ð½Ð°Ð´Ð¾
        disable_tqdm=False,

        # ðŸ”¥ ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð»Ð¸Ð½Ñƒ Ð¸ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ beam search Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ.
        generation_max_length=64,
        generation_num_beams=1,
    )

    return training_args


def create_data_collator(tokenizer, model):
    """
    Create data collator for sequence-to-sequence tasks.
    """
    return DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)


class InferenceOnlyTrainer(Seq2SeqTrainer):
    """
    Trainer, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹:
      - Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ,
      - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÐšÐÐ¡Ð¢ÐžÐœÐÐ«Ð™, Ð»ÐµÐ³ÐºÐ¸Ð¹ Ð¿Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸ evaluation-loop,
        Ð²Ð¼ÐµÑÑ‚Ð¾ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ð¾Ð³Ð¾ Trainer.evaluate() + Accelerate.
    """

    def train(self, *args, **kwargs):
        self.model.eval()
        return None

    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = "eval"):
        """
        ÐšÐ°ÑÑ‚Ð¾Ð¼Ð½Ð°Ñ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ evaluate, Ð¿Ð¾Ñ…Ð¾Ð¶Ð°Ñ Ð½Ð° Ñ‚Ð²Ð¾Ð¹ ÑÐºÑ€Ð¸Ð¿Ñ‚:
        - Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ DataLoader
        - Ñ€ÑƒÑ‡Ð½Ð¾Ð¹ Ð²Ñ‹Ð·Ð¾Ð² model.generate(...)
        - Ð¿Ð¾Ñ‚Ð¾Ð¼ compute_metrics(...)
        """

        # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚: ÐµÑÐ»Ð¸ ÑÐ²Ð½Ð¾ Ð¿ÐµÑ€ÐµÐ´Ð°Ð»Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐµÐ³Ð¾, Ð¸Ð½Ð°Ñ‡Ðµ self.eval_dataset
        if eval_dataset is None:
            eval_dataset = self.eval_dataset

        device = self.args.device
        self.model.to(device)
        self.model.eval()

        # ðŸ”§ Ð’ÐÐ–ÐÐž: ÑƒÐ´Ð°Ð»ÑÐµÐ¼ 'translation' Ð¸Ð· Ñ„Ð¸Ñ‡ÐµÐ¹ Ð¿ÐµÑ€ÐµÐ´ collate,
        # Ð¸Ð½Ð°Ñ‡Ðµ DataCollatorForSeq2Seq Ð»Ð¾Ð¼Ð°ÐµÑ‚ÑÑ Ð½Ð° nested dict.
        def collate_fn(features):
            if "translation" in features[0]:
                features = [
                    {k: v for k, v in f.items() if k != "translation"}
                    for f in features
                ]
            return self.data_collator(features)

        dataloader = DataLoader(
            eval_dataset,
            batch_size=self.args.per_device_eval_batch_size,
            collate_fn=collate_fn,
        )

        all_preds = []
        all_labels = []

        # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ Ñ†Ð¸ÐºÐ» Ð±ÐµÐ· Accelerate
        for batch in dataloader:
            labels = batch["labels"].clone()
            all_labels.append(labels)

            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            with torch.no_grad():
                generated_tokens = self.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=self.args.generation_max_length or 64,
                    num_beams=self.args.generation_num_beams or 1,
                )

            all_preds.append(generated_tokens.cpu())

        # ==== ÐŸÐÐ”Ð”Ð˜ÐÐ“ ÐŸÐ•Ð Ð•Ð” ÐšÐžÐÐšÐÐ¢Ð•ÐÐÐ¦Ð˜Ð•Ð™ ====
        pad_token_id = self.tokenizer.pad_token_id or 0
        ignore_index = -100

        # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ: Ð¿Ð°Ð´Ð´Ð¸Ð¼ pad_token_id
        max_pred_len = max(t.size(1) for t in all_preds)
        padded_preds = []
        for t in all_preds:
            pad_len = max_pred_len - t.size(1)
            if pad_len > 0:
                pad = torch.full(
                    (t.size(0), pad_len),
                    pad_token_id,
                    dtype=t.dtype,
                )
                t = torch.cat([t, pad], dim=1)
            padded_preds.append(t)
        preds_tensor = torch.cat(padded_preds, dim=0)

        # Ð›ÐµÐ¹Ð±Ð»Ñ‹: Ð¿Ð°Ð´Ð´Ð¸Ð¼ ignore_index (-100)
        max_label_len = max(t.size(1) for t in all_labels)
        padded_labels = []
        for t in all_labels:
            pad_len = max_label_len - t.size(1)
            if pad_len > 0:
                pad = torch.full(
                    (t.size(0), pad_len),
                    ignore_index,
                    dtype=t.dtype,
                )
                t = torch.cat([t, pad], dim=1)
            padded_labels.append(t)
        labels_tensor = torch.cat(padded_labels, dim=0)
        # ==== ÐšÐžÐÐ•Ð¦ ÐŸÐÐ”Ð”Ð˜ÐÐ“Ð ====

        preds_np = preds_tensor.numpy()
        labels_np = labels_tensor.numpy()

        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¹ Ð² Ð·Ð°Ð´Ð°Ð½Ð¸Ð¸ compute_metrics (ÐÐ• Ð¼ÐµÐ½ÑÐµÐ¼ ÐµÐ³Ð¾)
        metrics = compute_metrics((preds_np, labels_np), self.tokenizer)

        # ÐŸÑ€ÐµÑ„Ð¸ÐºÑÑƒÐµÐ¼ ÐºÐ»ÑŽÑ‡Ð¸, ÐºÐ°Ðº Ð´ÐµÐ»Ð°ÐµÑ‚ Trainer (test_bleu, eval_bleu Ð¸ Ñ‚.Ð¿.)
        metrics = {f"{metric_key_prefix}_{k}": v for k, v in metrics.items()}

        return metrics


def build_trainer(model, tokenizer, tokenized_datasets) -> Seq2SeqTrainer:
    """
    Build and return the trainer object for (pseudo) training and evaluation.
    """
    data_collator = create_data_collator(tokenizer, model)
    training_args: Seq2SeqTrainingArguments = create_training_arguments()

    return InferenceOnlyTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=lambda eval_preds: compute_metrics(eval_preds, tokenizer),
    )

# trainer.py

import torch
from torch.utils.data import DataLoader
from transformers import (
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)

from constants import OUTPUT_DIR
from evaluation import compute_metrics


def create_training_arguments() -> Seq2SeqTrainingArguments:
    """
    Create and return the training arguments for the model.

    Ð ÐµÐ¶Ð¸Ð¼ "inference only":
    - train() Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð´ÐµÐ»Ð°ÐµÑ‚ (ÑÐ¼. InferenceOnlyTrainer),
    - Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð½ÑƒÐ¶Ð½Ñ‹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ evaluate().
    """
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,

        # Ð¢Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð½Ðµ Ð±ÑƒÐ´ÐµÑ‚, Ð½Ð¾ Trainer Ð²ÑÑ‘ Ñ€Ð°Ð²Ð½Ð¾ Ð¿Ñ€Ð¾ÑÐ¸Ñ‚ ÑÑ‚Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹.
        num_train_epochs=1,
        max_steps=0,  # Ñ„Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸ "Ð½ÐµÑ‚ ÑˆÐ°Ð³Ð¾Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"

        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,

        learning_rate=2e-4,
        weight_decay=0.01,
        warmup_steps=0,

        logging_steps=1,
        save_steps=50,

        # Ð’ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐµ "Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ" Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ Ð½Ðµ Ð³Ð¾Ð½ÑÐµÐ¼
        evaluation_strategy="no",

        save_total_limit=1,
        load_best_model_at_end=False,

        metric_for_best_model="bleu",
        greater_is_better=True,

        max_grad_norm=1.0,

        # predict_with_generate Trainer'Ñƒ Ð¿Ð¾ ÑÑƒÑ‚Ð¸ Ð½Ðµ Ð½ÑƒÐ¶ÐµÐ½,
        # Ñ‚.Ðº. Ð¼Ñ‹ Ð´ÐµÐ»Ð°ÐµÐ¼ ÑÐ²Ð¾ÑŽ evaluate(), Ð½Ð¾ Ð¿ÑƒÑÑ‚ÑŒ Ð±ÑƒÐ´ÐµÑ‚.
        predict_with_generate=True,

        # Ð”Ð»Ñ 3.3B-Ð¼Ð¾Ð´ÐµÐ»Ð¸: fp16, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²Ð»ÐµÐ·Ñ‚ÑŒ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ
        fp16=True,
        gradient_accumulation_steps=1,

        # gradient_checkpointing Ð²Ñ‹ÐºÐ»ÑŽÑ‡ÐµÐ½, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð»Ð¾Ð²Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð½Ñ‹Ðµ Ð±Ð°Ð³Ð¸.
        gradient_checkpointing=False,

        dataloader_num_workers=4,

        # Ð§Ñ‚Ð¾Ð±Ñ‹ Trainer Ð½Ðµ Ð¿Ñ‹Ñ‚Ð°Ð»ÑÑ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð² WandB Ð¸ Ñ‚.Ð¿.
        report_to="none",

        # Ð¥Ð¾Ñ‚Ð¸Ð¼ Ð²Ð¸Ð´ÐµÑ‚ÑŒ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ, ÐµÑÐ»Ð¸ Ð½Ð°Ð´Ð¾
        disable_tqdm=False,

        # Ð­Ñ‚Ð¸ Ð¿Ð¾Ð»Ñ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ Ð² Ð½Ð°ÑˆÐµÐ¹ ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½Ð¾Ð¹ evaluate,
        # Ð½Ð¾ Ð¾ÑÑ‚Ð°Ð²Ð¸Ð¼ Ð¸Ñ… "Ñ€Ð°Ð·ÑƒÐ¼Ð½Ñ‹Ð¼Ð¸", Ð½Ð° ÑÐ»ÑƒÑ‡Ð°Ð¹ ÐµÑÐ»Ð¸ ÐºÑ‚Ð¾-Ñ‚Ð¾ Ð²Ñ‹Ð·Ð¾Ð²ÐµÑ‚
        # ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ evaluate() Ð±ÐµÐ· Ð¿ÐµÑ€ÐµÐ¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ.
        generation_max_length=256,
        generation_num_beams=4,
    )

    return training_args


def create_data_collator(tokenizer, model):
    """
    Create data collator for sequence-to-sequence tasks.
    """
    return DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)


class InferenceOnlyTrainer(Seq2SeqTrainer):
    """
    Trainer, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹:
      - Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ,
      - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÐšÐÐ¡Ð¢ÐžÐœÐÐ«Ð™, Ð»Ñ‘Ð³ÐºÐ¸Ð¹ Ð¿Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸ evaluation-loop,
        Ð²Ð¼ÐµÑÑ‚Ð¾ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ð¾Ð³Ð¾ Trainer.evaluate() + Accelerate.
    """

    def train(self, *args, **kwargs):
        # "Ð¤Ð¸ÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ" Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ: Ð¿Ñ€Ð¾ÑÑ‚Ð¾ ÑÑ‚Ð°Ð²Ð¸Ð¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² eval-Ñ€ÐµÐ¶Ð¸Ð¼.
        self.model.eval()
        return None

    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = "eval"):
        """
        ÐšÐ°ÑÑ‚Ð¾Ð¼Ð½Ð°Ñ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ evaluate, Ð±Ð»Ð¸Ð¶Ðµ Ðº Ð¿Ñ€Ð¾ÑˆÐ»Ð¾Ð¼Ñƒ multi_model_eval:

        - Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ DataLoader (Ð±ÐµÐ· Accelerate),
        - model.generate(...) Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ñ‹
          Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð»Ð¸ Ð´Ð»Ñ facebook/nllb-200-3.3B:
            * max_new_tokens=128
            * num_beams=4
            * do_sample=False
            * no_repeat_ngram_size=3
            * early_stopping=True
        - Ð·Ð°Ñ‚ÐµÐ¼ compute_metrics(...) Ð¸Ð· evaluation.py
        """

        # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚: ÐµÑÐ»Ð¸ ÑÐ²Ð½Ð¾ Ð¿ÐµÑ€ÐµÐ´Ð°Ð»Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐµÐ³Ð¾, Ð¸Ð½Ð°Ñ‡Ðµ self.eval_dataset
        if eval_dataset is None:
            eval_dataset = self.eval_dataset

        device = self.args.device
        self.model.to(device)
        self.model.eval()

        # ðŸ”§ Ð£Ð´Ð°Ð»ÑÐµÐ¼ 'translation' Ð¿ÐµÑ€ÐµÐ´ collate,
        # Ð¸Ð½Ð°Ñ‡Ðµ DataCollatorForSeq2Seq Ð»Ð¾Ð¼Ð°ÐµÑ‚ÑÑ Ð½Ð° nested dict.
        def collate_fn(features):
            if "translation" in features[0]:
                features = [
                    {k: v for k, v in f.items() if k != "translation"}
                    for f in features
                ]
            return self.data_collator(features)

        dataloader = DataLoader(
            eval_dataset,
            batch_size=self.args.per_device_eval_batch_size,
            collate_fn=collate_fn,
        )

        all_preds = []
        all_labels = []

        # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ Ñ†Ð¸ÐºÐ» Ð±ÐµÐ· Accelerate
        for batch in dataloader:
            # labels Ð¾ÑÑ‚Ð°ÑŽÑ‚ÑÑ Ð½Ð° CPU
            labels = batch["labels"].clone()
            all_labels.append(labels)

            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            with torch.no_grad():
                # â— ÐŸÐÐ ÐÐœÐ•Ð¢Ð Ð« Ð“Ð•ÐÐ•Ð ÐÐ¦Ð˜Ð˜, ÐœÐÐšÐ¡Ð˜ÐœÐÐ›Ð¬ÐÐž Ð‘Ð›Ð˜Ð—ÐšÐ˜Ð•
                # Ðš ÐŸÐ ÐžÐ¨Ð›ÐžÐœÐ£ multi_model_eval
                generated_tokens = self.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=128,
                    num_beams=4,
                    do_sample=False,
                    no_repeat_ngram_size=3,
                    early_stopping=True,
                )

            all_preds.append(generated_tokens.cpu())

        if not all_preds:
            return {}

        # ==== ÐŸÐÐ”Ð”Ð˜ÐÐ“ ÐŸÐ•Ð Ð•Ð” ÐšÐžÐÐšÐÐ¢Ð•ÐÐÐ¦Ð˜Ð•Ð™ ====
        pad_token_id = self.tokenizer.pad_token_id or 0
        ignore_index = -100

        # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ: Ð¿Ð°Ð´Ð´Ð¸Ð¼ pad_token_id
        max_pred_len = max(t.size(1) for t in all_preds)
        padded_preds = []
        for t in all_preds:
            pad_len = max_pred_len - t.size(1)
            if pad_len > 0:
                pad = torch.full(
                    (t.size(0), pad_len),
                    pad_token_id,
                    dtype=t.dtype,
                )
                t = torch.cat([t, pad], dim=1)
            padded_preds.append(t)
        preds_tensor = torch.cat(padded_preds, dim=0)

        # Ð›ÐµÐ¹Ð±Ð»Ñ‹: Ð¿Ð°Ð´Ð´Ð¸Ð¼ ignore_index (-100),
        # ÐºÐ°Ðº ÑÑ‚Ð¾ Ð´ÐµÐ»Ð°ÐµÑ‚ HuggingFace Trainer.
        max_label_len = max(t.size(1) for t in all_labels)
        padded_labels = []
        for t in all_labels:
            pad_len = max_label_len - t.size(1)
            if pad_len > 0:
                pad = torch.full(
                    (t.size(0), pad_len),
                    ignore_index,
                    dtype=t.dtype,
                )
                t = torch.cat([t, pad], dim=1)
            padded_labels.append(t)
        labels_tensor = torch.cat(padded_labels, dim=0)
        # ==== ÐšÐžÐÐ•Ð¦ ÐŸÐÐ”Ð”Ð˜ÐÐ“Ð ====

        preds_np = preds_tensor.numpy()
        labels_np = labels_tensor.numpy()

        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¹ Ð² Ð·Ð°Ð´Ð°Ð½Ð¸Ð¸ compute_metrics (ÐÐ• Ð¼ÐµÐ½ÑÐµÐ¼ ÐµÐ³Ð¾)
        metrics = compute_metrics((preds_np, labels_np), self.tokenizer)

        # ÐŸÑ€ÐµÑ„Ð¸ÐºÑÑƒÐµÐ¼ ÐºÐ»ÑŽÑ‡Ð¸, ÐºÐ°Ðº Ð´ÐµÐ»Ð°ÐµÑ‚ Trainer (test_bleu, eval_bleu Ð¸ Ñ‚.Ð¿.)
        metrics = {f"{metric_key_prefix}_{k}": v for k, v in metrics.items()}

        return metrics


def build_trainer(model, tokenizer, tokenized_datasets) -> Seq2SeqTrainer:
    """
    Build and return the trainer object for (pseudo) training and evaluation.
    """
    data_collator = create_data_collator(tokenizer, model)
    training_args: Seq2SeqTrainingArguments = create_training_arguments()

    return InferenceOnlyTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=lambda eval_preds: compute_metrics(eval_preds, tokenizer),
    )
